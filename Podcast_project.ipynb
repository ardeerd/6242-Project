{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from urllib.request import urlopen\n",
    "\n",
    "fields = ['country', 'rank','name', 'id', 'releaseDate', 'Primary_Genre', 'Artist']\n",
    "filename = \"top200_podcasts.csv\"\n",
    "\n",
    "with urlopen('https://rss.itunes.apple.com/api/v1/us/podcasts/top-podcasts/all/200/explicit.json') as data_file:    \n",
    "    data = json.load(data_file)\n",
    "\n",
    "results = data[\"feed\"]    \n",
    "allofthem = []\n",
    "cntr = 0\n",
    "\n",
    "for item in results[\"results\"]:\n",
    "    gnr = item[\"genres\"][0]\n",
    "    cntr += 1\n",
    "    x = ['us',cntr, item[\"name\"], item[\"id\"],  item[\"releaseDate\"],gnr[\"name\"], item[\"artistName\"]]\n",
    "    allofthem.append(x)\n",
    "\n",
    "\n",
    "with open(filename, 'w', newline ='',encoding=\"utf-8\") as csvfile: \n",
    "    writer = csv.writer(csvfile) \n",
    "    writer.writerow(fields) \n",
    "    writer.writerows(allofthem) \n",
    "################################   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "with urlopen('https://rss.itunes.apple.com/api/v1/ca/podcasts/top-podcasts/all/200/explicit.json') as data_file:    \n",
    "    data = json.load(data_file)\n",
    "\n",
    "results = data[\"feed\"]    \n",
    "allofthem = []\n",
    "cntr = 0\n",
    "\n",
    "for item in results[\"results\"]:\n",
    "    gnr = item[\"genres\"][0]\n",
    "    cntr += 1\n",
    "    x = ['ca',cntr, item[\"name\"], item[\"id\"],  item[\"releaseDate\"],gnr[\"name\"], item[\"artistName\"]]\n",
    "    allofthem.append(x)\n",
    "\n",
    "\n",
    "with open(filename, 'a', newline ='',encoding=\"utf-8\") as csvfile: \n",
    "    writer = csv.writer(csvfile) \n",
    "    writer.writerows(allofthem) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "with urlopen('https://rss.itunes.apple.com/api/v1/au/podcasts/top-podcasts/all/200/explicit.json') as data_file:    \n",
    "    data = json.load(data_file)\n",
    "\n",
    "results = data[\"feed\"]    \n",
    "allofthem = []\n",
    "cntr = 0\n",
    "\n",
    "for item in results[\"results\"]:\n",
    "    gnr = item[\"genres\"][0]\n",
    "    cntr += 1\n",
    "    x = ['au',cntr, item[\"name\"], item[\"id\"],  item[\"releaseDate\"],gnr[\"name\"], item[\"artistName\"]]\n",
    "    allofthem.append(x)\n",
    "\n",
    "\n",
    "with open(filename, 'a', newline ='',encoding=\"utf-8\") as csvfile: \n",
    "    writer = csv.writer(csvfile) \n",
    "    writer.writerows(allofthem) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "with urlopen('https://rss.itunes.apple.com/api/v1/uk/podcasts/top-podcasts/all/200/explicit.json') as data_file:    \n",
    "    data = json.load(data_file)\n",
    "\n",
    "results = data[\"feed\"]    \n",
    "allofthem = []\n",
    "cntr = 0\n",
    "\n",
    "for item in results[\"results\"]:\n",
    "    gnr = item[\"genres\"][0]\n",
    "    cntr += 1\n",
    "    x = ['uk',cntr, item[\"name\"], item[\"id\"],  item[\"releaseDate\"],gnr[\"name\"], item[\"artistName\"]]\n",
    "    allofthem.append(x)\n",
    "\n",
    "\n",
    "with open(filename, 'a', newline ='',encoding=\"utf-8\") as csvfile: \n",
    "    writer = csv.writer(csvfile) \n",
    "    writer.writerows(allofthem) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "with urlopen('https://rss.itunes.apple.com/api/v1/in/podcasts/top-podcasts/all/200/explicit.json') as data_file:    \n",
    "    data = json.load(data_file)\n",
    "\n",
    "results = data[\"feed\"]    \n",
    "allofthem = []\n",
    "cntr = 0\n",
    "\n",
    "for item in results[\"results\"]:\n",
    "    gnr = item[\"genres\"][0]\n",
    "    cntr += 1\n",
    "    x = ['in',cntr, item[\"name\"], item[\"id\"],  item[\"releaseDate\"],gnr[\"name\"], item[\"artistName\"]]\n",
    "    allofthem.append(x)\n",
    "\n",
    "filename = 'india_podcast.csv'\n",
    "with open(filename, 'a', newline ='',encoding=\"utf-8\") as csvfile: \n",
    "    writer = csv.writer(csvfile) \n",
    "    writer.writerows(allofthem) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NPR items are :::::::::::::::::::::::\n",
      "The Tim Dillon Show 1135137367 Tim Dillon 97\n"
     ]
    }
   ],
   "source": [
    "import podsearch\n",
    "#podcasts = podsearch.search(\"Deciding Decade\", country=\"us\", limit=1) 1525423535 \n",
    "podcasts = podsearch.search(\"1135137367\", country=\"us\", limit=1)\n",
    "\n",
    "#https://itunes.apple.com/lookup?id=1135137367&country=US&media=podcast&entity=podcastEpisode&limit=1\n",
    "    \n",
    "print (\"NPR items are :::::::::::::::::::::::\")\n",
    "\n",
    "for item in podcasts:\n",
    "    print (item.name, item.id, item.author, item.episode_count)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "973\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "all_podcasts = []\n",
    "with open(\"top200_podcasts.csv\", 'r') as csvfile:\n",
    "    csvreader = csv.reader(csvfile)\n",
    "    fields = next(csvreader)\n",
    "    \n",
    "    for row in csvreader:\n",
    "        all_podcasts.append(row)\n",
    "\n",
    "print (len(all_podcasts))    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "\n",
    "import podsearch\n",
    "import urllib.request\n",
    "from urllib.request import HTTPError\n",
    "\n",
    "episode_count = []\n",
    "fields = [ 'name', 'id', 'author', 'episode_count']\n",
    "filename = \"episode_info1.csv\"\n",
    "\n",
    "url_part1 = \"https://itunes.apple.com/lookup?id=\"\n",
    "url_part2 = \"&country=\"\n",
    "url_part3 = \"&media=podcast&entity=podcastEpisode&limit=1/json\"\n",
    "\n",
    "pp = 0\n",
    "for eachitem in temp:\n",
    "    pod_id = str(eachitem)\n",
    "    cntry = \"us\"\n",
    "    pp = pod_id\n",
    "    try:\n",
    "        url = url_part1 + pod_id + url_part2 + cntry + url_part3\n",
    "        #print (url)\n",
    "        with urlopen(url) as data_file:    \n",
    "            reviewdata = json.load(data_file)\n",
    "            x = reviewdata['results'] \n",
    "            xx = x[0]\n",
    "            tc = xx['trackCount'] \n",
    "            athr = xx['artistName']\n",
    "            name = xx['collectionName']\n",
    "       # for sub_item in podcasts:\n",
    "       #     x = [sub_item.name, sub_item.id, sub_item.author, sub_item.episode_count]\n",
    "            episode_count.append([name, pod_id ,athr,tc])\n",
    "    except Exception as err:\n",
    "        print (pp, err)\n",
    "        pass\n",
    "             \n",
    "with open(filename, 'w', newline ='') as csvfile: \n",
    "    writer = csv.writer(csvfile) \n",
    "    writer.writerow(fields) \n",
    "    writer.writerows(episode_count)\n",
    "################################  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import podsearch\n",
    "import urllib.request\n",
    "from urllib.request import HTTPError\n",
    "\n",
    "episode_count = []\n",
    "fields = [ 'name', 'id', 'author', 'episode_count']\n",
    "filename = \"episode_info.csv\"\n",
    "\n",
    "url_part1 = \"https://itunes.apple.com/lookup?id=\"\n",
    "url_part2 = \"&country=\"\n",
    "url_part3 = \"&media=podcast&entity=podcastEpisode&limit=1/json\"\n",
    "\n",
    "pp = 0\n",
    "for eachitem in all_podcasts:\n",
    "    pod_id = str(eachitem[3])\n",
    "    cntry = eachitem[0]\n",
    "    pp = pod_id\n",
    "    try:\n",
    "        url = url_part1 + pod_id + url_part2 + cntry + url_part3\n",
    "        #print (url)\n",
    "        with urlopen(url) as data_file:    \n",
    "            reviewdata = json.load(data_file)\n",
    "            x = reviewdata['results'] \n",
    "            xx = x[0]\n",
    "            tc = xx['trackCount'] \n",
    "            athr = xx['artistName']\n",
    "            name = xx['collectionName']\n",
    "       # for sub_item in podcasts:\n",
    "       #     x = [sub_item.name, sub_item.id, sub_item.author, sub_item.episode_count]\n",
    "            episode_count.append([name, pod_id ,athr,tc])\n",
    "    except Exception as err:\n",
    "        print (pp, err)\n",
    "        pass\n",
    "            \n",
    "print('sdfadsf = ',episode_count)\n",
    "\n",
    "with open(filename, 'w', newline ='') as csvfile: \n",
    "    writer = csv.writer(csvfile) \n",
    "    writer.writerow(fields) \n",
    "    writer.writerows(episode_count)\n",
    "################################   \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from urllib.request import urlopen\n",
    " \n",
    "with urlopen('https://rss.itunes.apple.com/api/v1/us/podcasts/top-podcasts/all/200/explicit.json') as data_file:    \n",
    "    data = json.load(data_file)\n",
    "\n",
    "results = data[\"feed\"]    \n",
    "allofthem = []\n",
    "cntr = 0\n",
    "podcastIDs = []\n",
    "\n",
    "for item in results[\"results\"]:\n",
    "    gnr = item[\"genres\"][0]\n",
    "    cntr += 1\n",
    "    x = ['us',cntr, item[\"name\"], item[\"id\"],  item[\"releaseDate\"],gnr[\"name\"], item[\"artistName\"]]\n",
    "    allofthem.append(x)\n",
    "    podcastIDs.append(item[\"id\"])\n",
    "print (podcastIDs) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "973\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "filename = 'C:/OMSA/CS6242/Project/6242-Project/top200_podcasts.csv'\n",
    "fields = []\n",
    "podcastIDs = []\n",
    "with open(filename, 'r') as csvfile:\n",
    "    csvreader = csv.reader(csvfile)\n",
    "    fields = next(csvreader)\n",
    "    \n",
    "    for row in csvreader:\n",
    "        podcastIDs.append(row)\n",
    "print (len(podcastIDs))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['us', '1', 'Mommy Doomsday', '1540849480', '2021-02-09', 'True Crime', 'NBC News']\n"
     ]
    }
   ],
   "source": [
    "print (podcastIDs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from urllib.request import urlopen\n",
    "\n",
    "#https://itunes.apple.com/us/rss/customerreviews/id=1222114325/json\n",
    "#findreviews = \"https://itunes.apple.com/us/rss/customerreviews/id=\" + qq + \"/json\"\n",
    "#findreviews = \"https://itunes.apple.com/us/rss/customerreviews/page=1/id=\" + qq + \"/sortby=mostrecent/json\"\n",
    "#https://itunes.apple.com/lookup?id=1135137367&country=US&media=podcast&entity=podcastEpisode&limit=1\n",
    "\n",
    "     \n",
    "rows = []\n",
    "for i in podcastIDs[836:]:    \n",
    "    ii = i[3]\n",
    "    findreviews = \"https://itunes.apple.com/in/rss/customerreviews/id=\" + str(ii) + \"/json\"\n",
    "    #now lets get the second page as well\n",
    "    #findreviews = \"https://itunes.apple.com/uk/rss/customerreviews/page=1/id=\" +  str(ii) + \"/sortby=mostrecent/json\"\n",
    "    #print (i, findreviews)\n",
    "    with urlopen(findreviews) as data_file:    \n",
    "        reviewdata = json.load(data_file)\n",
    "        reviewdatafeed = reviewdata[\"feed\"] \n",
    "        for key in reviewdatafeed.keys():\n",
    "            rvwHeading = []\n",
    "            rvwText =[]\n",
    "\n",
    "            try:\n",
    "                qq = reviewdatafeed[\"entry\"]\n",
    "                for eachitem in qq:\n",
    "                    rvwHeading.append(str(eachitem[\"title\"][\"label\"]))\n",
    "                    rvwText.append((eachitem[\"content\"][\"label\"]))\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "        x = [i, 'in', rvwHeading, rvwText]\n",
    "        rows.append(x)\n",
    "    \n",
    "    \n",
    "    findreviews = \"https://itunes.apple.com/in/rss/customerreviews/page=2/id=\" +  str(ii) + \"/sortby=mostrecent/json\"\n",
    "    # print (i, findreviews)\n",
    "    with urlopen(findreviews) as data_file:    \n",
    "        reviewdata = json.load(data_file)\n",
    "        reviewdatafeed = reviewdata[\"feed\"] \n",
    "        for key in reviewdatafeed.keys():\n",
    "            rvwHeading = []\n",
    "            rvwText =[]\n",
    "\n",
    "            try:\n",
    "                qq = reviewdatafeed[\"entry\"]\n",
    "                for eachitem in qq:\n",
    "                    rvwHeading.append(str(eachitem[\"title\"][\"label\"]))\n",
    "                    rvwText.append((eachitem[\"content\"][\"label\"]))\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "        x = [i, 'in', rvwHeading, rvwText]\n",
    "        rows.append(x)\n",
    "        \n",
    "#############################\n",
    "import csv\n",
    "fields = ['id', 'country', 'review_title', 'review_text']\n",
    "filename = \"review_text.csv\"\n",
    "with open(filename, 'a', newline ='',encoding=\"utf-8\") as csvfile: \n",
    "    writer = csv.writer(csvfile) \n",
    "    writer.writerow(fields) \n",
    "    writer.writerows(rows)\n",
    "################################   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you',\n",
    "             \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', \n",
    "             'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\",\n",
    "             'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', \n",
    "             'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being',\n",
    "             'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or',\n",
    "             'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', \n",
    "             'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on',\n",
    "             'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why',\n",
    "             'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not',\n",
    "             'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \n",
    "             \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn',\n",
    "             \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma',\n",
    "             'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn',\n",
    "             \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\",\n",
    "             'a', 'able', 'about', 'across', 'after', 'all', 'almost', 'also', 'am', 'among', 'an', 'and', 'any',\n",
    "              'are', 'as', 'at', 'be', 'because', 'been', 'but', 'by', 'can', 'cannot', 'could', 'dear', 'did',\n",
    "              'do', 'does', 'either', 'else', 'ever', 'every', 'for', 'from', 'get', 'got', 'had', 'has', 'have',\n",
    "              'he', 'her', 'hers', 'him', 'his', 'how', 'however', 'i', 'if', 'in', 'into', 'is', 'it', 'its',\n",
    "              'just', 'least', 'let', 'like', 'likely', 'may', 'me', 'might', 'most', 'must', 'my', 'neither',\n",
    "              'no', 'nor', 'not', 'of', 'off', 'often', 'on', 'only', 'or', 'other', 'our', 'own', 'rather',\n",
    "              'said', 'say', 'says', 'she', 'should', 'since', 'so', 'some', 'than', 'that', 'the', 'their',\n",
    "              'them', 'then', 'there', 'these', 'they', 'this', 'tis', 'to', 'too', 'twas', 'us', 'wants',\n",
    "              'was', 'we', 'were', 'what', 'when', 'where', 'which', 'while', 'who', 'whom', 'why', 'will',\n",
    "              'with', 'would', 'yet', 'you', 'your', 'please', 'podcast', 'love', 'episode', 'host', 'recommend',\n",
    "              'great', 'never', 'continue','review', 'reviewers','series', 'listen', 'car', 'looking', 'forward',\n",
    "              'interviews', 'thanks', 'amazing', 'great', 'show', 'awesome', 'sound', 'podcasts','even', 'though',\n",
    "              'story', 'episodes', 'stories', 'audiobook', 'entire', 'want'\n",
    "            ]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    clean_text = text.lower()\n",
    "    clean_text = ''.join([c for c in clean_text if c.isalpha() or c.isspace()])\n",
    "    return clean_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_excerpts(excerpts):\n",
    "   # \"clean_excerpts expects a list of strings as input\"\n",
    "    clean_excerpts = [clean_text(e) for e in excerpts]\n",
    "    return clean_excerpts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'', 'makes', 'fast', 'black', 'white', 'putting', 'phenomenal', 'enough', 'enticing', 'turns', 'choking', 'factual', 'sooo', 'travel', 'enjoying', 'edge', 'background', 'interested', 'rican', 'diving', 'suspense', 'hear', 'well', 'seems', 'arranged', 'storyteller', 'breathless', 'wondry', 'feel', 'visualize', 'etc', 'thank', 'play', 'absolutely', 'delicious', 'interesting', 'brought', 'something', 'straight', 'intriguing', 'descriptive', 'anyone', 'excellent', 'little', 'good', 'obvious', 'nope', 'asian', 'accompany', 'anxious', 'appointment', 'couldnât', 'one', 'reporting', 'scenes', 'make', 'narration', 'race', 'beautifully', 'new', 'thailand', 'two', 'captivating', 'best', 'top', 'come', 'mai', 'reading', 'hooked', 'itâs', 'production', 'coming', 'canât', 'brilliantly', 'made', 'thought', 'experience', 'puerto', 'project', 'tears', 'literal', 'imagery', 'making', 'thrilling', 'pay', 'warning', 'engrossed', 'tales', 'brings', 'iâll', 'countless', 'seat', 'thoughts', 'cried', 'parts', 'wonât', 'book', 'men', 'many', 'survival', 'ugly', 'voices', 'put', 'really', 'everything', 'world', 'details', 'chiang', 'seeing', 'way', 'people', 'following', 'actually', 'last', 'remember', 'soccer', 'boys', 'guys', 'notch', 'store', 'deadly', 'happening', 'find', 'pulled', 'design', 'know', 'always', 'spellbinding', 'enthralling', 'action', 'crisp', 'involved', 'right', 'keeps', 'start', 'write', 'disappointed', 'presented', 'need', 'whole', 'belong', 'believe', 'getgo', 'researched', 'nice', 'perspective', 'invarious', 'price', 'years', 'quality', 'constantly', 'work', 'stop', 'suspenseful', 'still', 'doesnât', 'danger', 'written', 'divers', 'back', 'behind', 'riveted', 'released', 'near', 'shed', 'ending', 'incl', 'reviews', 'incredible', 'promise', 'hokey', 'talking', 'hearing', 'survivalism', 'portrayal', 'counting', 'firsthand', 'far', 'wanted', 'disappears', 'fan', 'heard', 'donât', 'plus', 'part', 'clean', 'real', 'thing', 'drawn', 'beautiful', 'engaged', 'storytelling', 'skill', 'future', 'maybe', 'true', 'hurry', 'first', 'interest', 'portrait', 'perseverance', 'thats', 'sort', 'sad', 'another', 'situations', 'brink', 'told', 'remains', 'listened', 'already', 'next', 'sit', 'writing', 'media', 'lots', 'finish', 'coach', 'effects', 'wait', 'worth', 'perfectly', 'telling', 'fourpart', 'thai', 'excited', 'lot', 'big', 'days', 'news', 'yes', 'throat', 'clear', 'rarely', 'life', 'perfect', 'step', 'hour', 'noise', 'jetblueivy', 'iâm', 'omgseriously', 'give', 'end', 'humanity', 'thatâs', 'hahaha', 'bit', 'moving', 'assuming', 'iâve', 'cave', 'rai', 'areastherefore', 'outlets', 'narrator', 'emotions', 'bringing', 'particular', 'backstorymost', 'saying', 'characters', 'intense', 'nothing', 'âœstoryâ', 'mattered', 'vivid', 'experiencing', 'ends', 'fashion', 'meant', 'along', 'disappoint', 'recognized', 'gone', 'rescue', 'edit', 'extremely', 'walk', 'four', 'imagine', 'ripoff', 'choked', 'lives', 'engrossing', 'downplays', 'girlfriends', 'sure', 'wonderous', 'gonna', 'rest', 'hopefully', 'especially', 'wondery', 'moved', 'name', 'literally', 'sorry', 'thoughtfully', 'crying', 'whatsoever', 'live', 'misadventures', 'praised', 'today', 'seem', 'peopleâs', 'compelling', 'sai', 'beginning', 'time', 'together', 'done', 'agree', 'phuket', 'society', 'superheroes', 'acting', 'listening', 'definitely', 'turn', 'sounds', 'experienced', 'couple', 'head', 'experts'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "word_list = ['I love this podcast! The storytelling is so enthralling, and Iâ€™m always on the edge of my seat even though I already know how the rescue ends. The only thing I have to say about this podcast is that it canâ€™t come fast enough! 100% definitely recommend to anyone interested in survivalism', 'Excellent storytelling and very suspenseful. Really enjoying it.', 'Brilliantly made!', 'Spellbinding!', 'Hopefully it doesnâ€™t turn into another Wondry project like one plus one where it was a great series, and then 75% of it disappears and they never continue it.', 'Episode 3 brought me to literal tears.  I couldnâ€™t even listen in the store and I had to sit in my car to finish. I remember this and have always wanted to know more of the story. I was disappointed to find only one more episode remains!  Thanks for a great podcast about this amazing story.', 'Well researched, amazing storyteller, lots of new details, real life superheroes. Even though I know how it ends,  I cried like other reviewers while on my walk. Hurry up with more episodes!', 'Great podcast a must listen', 'Amazing to hear this story from the beginning and with all of the details.', 'I love true stories, and have a particular interest in survival tales. I was drawn in by the first four-part series on the Thai cave rescue, which I listen to on Wondery. Looking forward to hearing future stories of perseverance and survival.', 'This is a podcast. My throat was choked on suspense most of the time and the rest of the time is was choking back tears. Such a well researched, well presented podcast. Great storytelling. 10/10', 'I was really looking forward to this podcast- interviews with those involved etc. But nope! Just another audiobook by Wondery. Thanks no thanks.', 'Iâ€™m on episode 2, near the end.  There donâ€™t seem to be any interviews or voices other than the narrator.  Itâ€™s just the narrator talking and sort of acting out other peopleâ€™s voices.', 'So good!', 'This is so well done. We all heard this sorry when it was happening but now we know the details. So good!', 'Great story telling. Even when you know the story. Thank You', 'I know it ends well, it is in the name... But I still cried a bit and was anxious listening. Great narration.', 'This podcast has some phenomenal storytelling and always has me on the edge of my seat. This is a GREAT listen!', 'Appointment listening', 'What an amazing story! One of the best put together podcasts Iâ€™ve heard! I canâ€™t wait for the next episode, amazing work.', 'SO intriguing, interesting, sound effects are awesome, Iâ€™m hooked. Canâ€™t wait for the next episode!\\n*Warning* you may shed a few tears', 'Amazing podcast!!!!', 'It is so enticing to listen to, the noise and sound effects are perfectly made to make a perfect portrait that you can imagine in your head. I love all the wondery podcasts but this one is by far the best', 'So descriptive, great background sounds to accompany the storytelling. You can visualize the entire story.  Itâ€™s intense but not hokey. Looking forward to all the stories.', 'This is so thoughtfully done and suspenseful. I need the next two episodes!!', 'Amazing story!  Well arranged and clear portrayal.  Excited to hear last two episodes!', 'Iâ€™m ugly crying already and I already know how it turns out. The story telling is amazing!', 'I like my podcasts to be more factual. Not told like a story book and making up or assuming parts of the â€œstoryâ€', 'This is top quality storytelling. Thrilling, captivating story.', 'Really a big fan of Wondery and ALL of their podcasts! Great story telling and keeps me on the edge of my seat!', 'Keeps me riveted and engaged!', 'So well done. Not sure why Iâ€™m constantly on the brink of tears. This is incredible, maybe thatâ€™s why! Beautifully done and a beautiful story', 'I want more and more this podcast well thought of.', 'This is extremely well done. Well researched and the story is told in a crisp and clean fashion that keeps you moving along wonderous and breathless for the next step.', 'The story telling is amazing!  While listening I feel like Iâ€™m seeing and experiencing everything firsthand.', 'Wondery never seems to disappoint.  Excellent writing and sound design as always.', 'I feel like Iâ€™m experiencing the story right along with the characters. Incredible and such vivid imagery. Worth the listen!', 'I live in Thailand actually in Phuket and know Mai Sai and my girlfriends from Chiang Rai and I live near Chiang Mai for four years. I still travel in those areas.Therefore, it is very nice to get the backstory.Most people (incl me) only heard about the obvious parts or what  made it inVarious media outlets.', 'What do I have to pay to listen to the rest of the episodes? Iâ€™ll pay any price! Hahaha.  I canâ€™t wait!!! I remember following the story on the news in real time, but to know the behind the scenes details, delicious!', 'I was pulled in from the get-go. The production of this podcast is top notch. Amazing sound design and the host is great too. GREAT WORK!!!', 'Got me on the edge of my seat. Great show', 'Awesome', 'This is written and told very well. I know the ending but am still on the edge of my seat listening.', 'I listen to a lot of podcasts, but rarely am moved to write a review. This is SO WELL DONE.  The reporting is excellent and the storytelling is excellent. So suspenseful and compelling. I couldnâ€™t stop listening.  Iâ€™m just sad I have to wait for the next episode- I want it now!', 'Absolutely engrossing! Love this podcast and canâ€™t wait to hear more', 'I was looking for something new to listen to and I canâ€™t get enough of this. Looking forward to hearing from them in their next episode', 'This is a straight ripoff of Deadly Misadventures.', 'OMG....SERIOUSLY ONE OF THE BEST PODCAST I HAVE EVER HEARD!!!  I am sooo engrossed in the story that by the time an episode is over I canâ€™t believe almost an hour has gone by and I literally find myself being sad that itâ€™s over and start counting down the days until the next one is released.  Iâ€™ve only experienced thoughts and emotions I get from listening to this podcast from one other one and THATS SAYING SOMETHING as Iâ€™ve listened to countless!!!  The way the story is told brings you into the action from EVERY perspective and makes you feel as if you were actually there experiencing what they are experiencing!!!  Give it a listen I promise you wonâ€™t be disappointed!!!\\n\\nEdit after reading some of the other reviews: PLEASE PLEASE donâ€™t be like a couple of the reviewers on here (jetblueivy) making this story about RACE?!?  LIKE WHAT???  RACE???  To me this story is about the WHOLE WORLD coming together putting there own lives in danger to rescue some little boys and their soccer coach and race does not play a part not whatsoever ITS ABOUT ALL HUMANITY GUYS COME ON!!! Yes the two men who were the experts and had the experience in cave diving to rescue the boys were white but that meant NOTHING!!! It would not have mattered if the divers were black Asian Puerto Rican etc. they would have been praised and recognized all the same because it was about their SKILL NOTHING ELSE!!!\\n\\nI agree there are many situations in society today that race does play a part but this story is not one of them!!! Please people stop bringing race into situations where it does not belong especially because it downplays other situations where it does thatâ€™s all Iâ€™m gonna say.', 'Great storytelling itâ€™s like listening to a book. Canâ€™t wait for the next episode!', 'Must listen']\n",
    "\n",
    "www = []\n",
    "for line in word_list:\n",
    "    for w in line.split():\n",
    "        ww = clean_text(w)\n",
    "        if ww not in stopwords:\n",
    "            www.append( ww)\n",
    "print (set(www))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
